---
title: "Analysing Patient Data"
teaching: 40
exercises: 20
questions:
- How can I process tabular data files in Python?
objectives:
- Explain what a library is and what libraries are used for.
- Import a Python library and use the functions it contains.
- Read tabular data from a file into a program.
- Select individual values and subsections from data.
- Perform operations on arrays of data.
keypoints:
- 'Describe data with pandas: `mean`, `median`, `std`, `var`, `sum`, `count`, `value_counts`.'
- Use `numeric_only=True` for DataFrame-wide reductions and `axis=` to switch between column-wise (`axis=0`) and row-wise (`axis=1`).
- Combine boolean masks with summaries to analyze filtered subsets.
- Handle missing data with `dropna()`; reorder results with `sort_index()`/`reindex()` and `sort_values()`/`reset_index()`.
- Explore categories with `unique()` and summarize groups with `groupby()`.
---

{{< include ../_includes/header.qmd >}}

Words tell a story; statistics help us check whether that story fits the data. Once the Metabric data are loaded and labelled, the next step is to turn questions into short, reproducible operations: filter the rows you care about, pick the variables that answer the question, and apply the right summary or comparison.


Before we can analyze patient data, we need to load it into Python in a way that makes it easy to work with. Let’s load the Metabric patient data and set the patient ID as the index, so rows are labelled meaningfully. If you already have `metabric_patients` in memory from a previous episode, you can reuse it; otherwise, run the following to (re)load it:

```{python}
import pandas as pd

metabric_patients = pd.read_csv(
    'https://zenodo.org/record/6450144/files/metabric_clinical_and_expression_data.csv',
    index_col='Patient_ID'
)
```

## Statistics on Data

Before jumping into complex models, we start by describing the data. Descriptive statistics answer simple questions like “what is typical?” and “how much do values vary?” These summaries help you build intuition and spot problems early (e.g., unexpected scales or missing values).

The pandas library provides a wide range of statistical functions and methods to compute summary statistics for your data. Below provides some of the key statistical measures you can compute using this library. 


### Mean 

Mean (*average*) of a single column (one variable):

```{python}
metabric_patients.loc[:, 'Survival_time'].mean()
```

This answers “on average, what is the value of this variable?” Here we use the `Survival_time` column.

Mean of all numeric columns (column-wise):

```{python}
metabric_patients.mean(numeric_only = True)
```

This computes the mean for each numeric column independently, giving you a quick overview of typical values across the dataset.

::: {.callout-note}
## Why `numeric_only=True`?

Many DataFrames mix numeric columns (like Tumour_size) with non-numeric columns (like Patient_ID or Cohort labels). Adding `numeric_only=True` tells pandas to ignore non-numeric columns when computing a statistic across the whole DataFrame. This avoids errors or confusing results. For a single numeric column (e.g., `metabric_patients.loc[:, 'Survival_time']`), you don’t need this argument; it’s most helpful for DataFrame-wide operations.
:::

Mean across each row:

```{python}
metabric_patients.mean(axis=1, numeric_only=True)
```


This treats each row as a small collection of values and averages across columns. It only makes sense when the columns you include are on a comparable scale (for example, a set of related measurements). The `axis=1` argument tells Pandas to compute across columns for each row.

::: {.callout-note}
## Understanding `axis=`

- `axis=0` (default) means “by column”: compute one result per column across rows.
- `axis=1` means “by row”: compute one result per row across columns.

Most reducers (`sum`, `mean`, `min`, `max`, etc.) accept `axis=` and follow the same pattern.
:::

### Median

The median (*robust "typical" value*) is the middle value when data are sorted. It is not affected by extreme values (outliers) and is often used to describe the central tendency of data. 

```{python}
metabric_patients.median(numeric_only = True)
```

Visualising mean vs median:

```{python}
#| echo: false
#| fig-align: center
import matplotlib.pyplot as plt

# choose a numeric column to visualise
vals = metabric_patients.loc[:, 'Tumour_size'].dropna()

plt.figure(figsize=(6, 3.5))
plt.hist(vals, bins=30, color='#8ecae6', edgecolor='white')
plt.axvline(vals.mean(), color='#d62828', linestyle='--', linewidth=2, label='Mean')
plt.axvline(vals.median(), color='#f77f00', linestyle='-', linewidth=2, label='Median')
plt.title('Tumour size distribution')
plt.xlabel('Tumour size (mm)')
plt.ylabel('Count')
plt.legend()
plt.tight_layout()
```

The mean pulls toward extreme values more than the median. In skewed distributions, the two lines will sit apart.

### Standard Deviation 

The standard deviation (*spread around the mean*) measures the amount of variation or dispersion in a dataset. A lower standard deviation indicates that data points are close to the mean, while a higher standard deviation indicates greater variability. 

```{python}
metabric_patients.std(numeric_only = True)
```

### Variance 

Variance (*spread in squared units*) is the square of the standard deviation. It quantifies how much individual data points deviate from the mean. It’s useful mathematically, but note its units are squared (e.g., cm² if the original variable is in cm). 

```{python}
metabric_patients.var(numeric_only = True)
```

### Sum 

Use `.sum()` to add up values.

Total for each numeric column (column-wise):

```{python}
metabric_patients.sum(numeric_only = True)
```

Total for a single column (e.g., Mutation_count across all patients):

```{python}
metabric_patients.loc[:, 'Mutation_count'].sum()
```

If you ever sum across rows (`axis=1`), be sure the columns are on comparable scales.

Row-wise sum of selected numeric columns (use with care):

```{python}
metabric_patients.loc[:, 'ESR1':'MLPH'].sum(axis = 1, numeric_only = True)
```

What the output shows

- DataFrame sum (no `axis=` given): a Series (one total per numeric column).
- Single-column sum: a single number.
- Row-wise sum (`axis=1`): a Series indexed by row labels (one total per patient).

### Count 

Use `.count()` to count non-missing values. This is helpful to see how complete each column is.

Count of non-missing values for each column:

```{python}
metabric_patients.count()
```

This returns the counts or the number of entries excluding missing values for each column in the dataframe. 

Count of non-missing values for a single column:

```{python}
metabric_patients.loc[:, 'Survival_status'].count()
```

This returns a single number - non-missing entries in the Survival_status column.

Here is a quick reference summary table of common useful functions.

```{=html}
<table>
  <thead>
    <tr>
      <th>Function</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td align="center"><code>count</code></td>
      <td>Number of non-NA observations</td>
    </tr>
    <tr>
      <td align="center"><code>sum</code></td>
      <td>Sum of values</td>
    </tr>
    <tr>
      <td align="center"><code>mean</code></td>
      <td>Mean of values</td>
    </tr>
    <tr>
      <td align="center"><code>median</code></td>
      <td>Arithmetic median of values</td>
    </tr>
    <tr>
      <td align="center"><code>min</code></td>
      <td>Minimum</td>
    </tr>
    <tr>
      <td align="center"><code>max</code></td>
      <td>Maximum</td>
    </tr>
    <tr>
      <td align="center"><code>mode</code></td>
      <td>Mode</td>
    </tr>
    <tr>
      <td align="center"><code>abs</code></td>
      <td>Absolute Value</td>
    </tr>
    <tr>
      <td align="center"><code>prod</code></td>
      <td>Product of values</td>
    </tr>
    <tr>
      <td align="center"><code>std</code></td>
      <td>Bessel-corrected sample standard deviation</td>
    </tr>
    <tr>
      <td align="center"><code>var</code></td>
      <td>Unbiased variance</td>
    </tr>
    <tr>
      <td align="center"><code>sem</code></td>
      <td>Standard error of the mean</td>
    </tr>
    <tr>
      <td align="center"><code>skew</code></td>
      <td>Sample skewness (3rd moment)</td>
    </tr>
    <tr>
      <td align="center"><code>kurt</code></td>
      <td>Sample kurtosis (4th moment)</td>
    </tr>
    <tr>
      <td align="center"><code>quantile</code></td>
      <td>Sample quantile (value at %)</td>
    </tr>
    <tr>
      <td align="center"><code>cumsum</code></td>
      <td>Cumulative sum</td>
    </tr>
    <tr>
      <td align="center"><code>cumprod</code></td>
      <td>Cumulative product</td>
    </tr>
    <tr>
      <td align="center"><code>cummax</code></td>
      <td>Cumulative maximum</td>
    </tr>
    <tr>
      <td align="center"><code>cummin</code></td>
      <td>Cumulative minimum</td>
    </tr>
  </tbody>
</table>
```

In summary, descriptive statistics provide a powerful first look at your data, helping you understand its central tendencies, variability, and completeness. By using functions like `mean`, `median`, `std`, and `value_counts`, you can quickly identify patterns, spot anomalies, and prepare your dataset for deeper analysis. Mastering these basic tools in pandas will make your data exploration more efficient and insightful.

Recall from the previous episode that you can select specific rows using boolean conditions (masks). See the section on [boolean masks](3_Loading_Data.qmd#boolean-masks). We’ll apply the above statistics to filtered subsets. All the statistical operators that work on entire DataFrames work the same way on slices.

Below are a few examples showing how to compute min and max over columns and rows, and how to find which patient or column produced that value.

Let's say we want to analyze patients from cohort 1, whose IDs range from MB-0000 to MB-0906. For example, to find the maximum tumour size among these patients, we can use the following code:

```{python}
cohort_mask = metabric_patients.loc[:, "Cohort"] == 1

print('Maximum for Tumour_size column for cohort 1 patients:')
print(metabric_patients.loc[cohort_mask, "Tumour_size"].max())
```

For the same set of patients, you can check the minimum mutation count and identify which patient has it. For example:

```{python}
print('\nMinimum for Mutation_count column for cohort 1 patients:')
print(metabric_patients.loc[cohort_mask, "Mutation_count"].min())
```

## Useful Helper Functions

Before plotting or building more complex summaries, a few small helpers make your life easier. These tools don’t change the meaning of your data—they help you clean rows, control order, and ask focused questions. After each command, check what changed: the values, the order, the labels, or the row count.

### Missing Values

It’s important to make sure your data doesn’t contain missing values (represented as `NaN`, which stands for "Not a Number"). Missing values can cause errors or misleading results in your analysis. For example, calculating the mean of a column with missing values will return NaN, and plotting functions might skip or misrepresent those data points.

To handle this, you can use the `dropna()` method. This method removes any rows (or columns) that contain missing values. By default, `dropna()` removes rows where any column has a missing value.

Drop rows where either '3-gene_classifier' or 'GATA3' is missing:

```{python}
subset = metabric_patients.loc[:, ['3-gene_classifier', 'GATA3']].dropna()
print('Rows before:', len(metabric_patients))
print('Rows after dropna:', len(subset))
```

You’ll see the original row count and a smaller count after `dropna()` command. The `subset` DataFrame keeps only rows where both columns are present. By default, rows are dropped if any of the specified columns are missing. To drop columns instead of rows, use `axis=1`. To control which columns are checked, pass `subset=[...]` (as above).

### Frequency of Values 

To inspect how often each distinct value appears in a column, use `value_counts()`. This is particularly helpful for categorical variables (for example, labels) to check balance, spot rare categories, and reveal unexpected or missing labels. By default it returns counts sorted by frequency.

```{python}
metabric_patients.value_counts()
```

When you call `value_counts()` on an entire DataFrame, each index represents a unique combination of values across all columns, and the count shows how many rows share that exact combination. Since each row in our dataset is unique, all counts will be 1—this is a quick way to check for duplicate rows.

For a specific column (e.g., `3-gene_classifier`):

```{python}
metabric_patients.loc[:, '3-gene_classifier'].value_counts()
```

When used on a single column, `value_counts()` returns a single column dataframe (or Series) where the index lists each unique value (such as category names) and the corresponding values show how many times each occurs. In the example above, you’ll see four categories and their respective counts.

Set `normalize=True` to display proportions (percentages) rather than raw counts.

```{python}
metabric_patients.loc[:, '3-gene_classifier'].value_counts(normalize=True)
```

To display percentages, multiply the result by 100:

```{python}
metabric_patients.loc[:, '3-gene_classifier'].value_counts(normalize=True) * 100
```

By default, missing values (or `NaN`) are excluded from the result. This means that only rows with actual, non-missing values are counted in the frequency table. If you want to include missing values as their own category in the output, you can pass the argument `dropna=False` to `value_counts()`. This will show how many times `NaN` appears in the column, which is useful for quickly assessing the amount of missing data in a categorical variable.

```{python}
metabric_patients.loc[:, '3-gene_classifier'].value_counts(dropna=False)
```

::: {.callout-tip}
## Tip — Missing Categories

Use `dropna=False` to see how many entries are missing. If your analysis should exclude missing rows entirely, clean first with `.dropna()` on the relevant columns (see the Missing Values section above) and then run `value_counts()`.
:::

Counts returned by `value_counts()` are automatically sorted from most to least frequent. If you want to sort the counts by the actual labels (in alphabetical order), you can use the `sort_index()` method. This is useful when you need to view your data categories in a predictable, ordered way rather than by their frequency.

```{python}
metabric_patients.loc[:, 'Integrative_cluster'].value_counts().sort_index()
```

To display categories in a custom order (such as a clinically meaningful sequence), use the `reindex()` method. This lets you specify the exact order of labels you want to see in your output, rather than relying on alphabetical or frequency-based sorting. For example, if you have a desired order of cluster labels, you can filter to only those present in your data and then reindex the counts:

```{python}
counts = metabric_patients.loc[:, 'Integrative_cluster'].value_counts()

# custom order (keep only labels that exist)
desired = ["1", "2", "3", "4ER-", "4ER+", "5", "6", "7", "8", "9", "10"]
present = [lab for lab in desired if lab in counts.index]
counts_custom = counts.reindex(present)
counts_custom
```

This approach first uses `value_counts()` to tally each unique value in the Integrative_cluster column, producing a pandas Series with cluster labels as the index and their counts as values. The `desired` list defines the preferred order for these labels.

The code then filters `desired` to include only labels that are actually present in the data. Using `reindex(present)` rearranges the counts Series to match this custom order, ensuring the output is organized as intended.

If a label from `desired` is missing in the data, it is simply omitted from the result—no NaNs are introduced. This method is especially helpful for displaying or plotting results in a meaningful sequence.



### Sorting numeric values (and resetting the index)

Use `sort_values()` to order numeric values. After sorting a Series, the original row labels (patient IDs) stay attached. Use `reset_index(drop=True)` if you want a clean 0..N-1 index for display.


To order values from low to high, use `sort_values()`. After sorting a Series, the original row labels stay attached; use `reset_index(drop=True)` to create a clean 0..N-1 index.

```{python}
ages = metabric_patients.loc[:, 'Age_at_diagnosis']
ages_sorted = ages.sort_values()
ages_sorted_head = ages_sorted.reset_index(drop=True).head()
ages_sorted_head
```

What the output shows

- `ages_sorted` keeps the original labels so you can trace back to the patient.
- After `reset_index(drop=True)`, the result starts at 0, which is handy for quick previews or exporting.

### Unique Values

Use `unique()` to list the distinct values observed. This returns a NumPy array. For a tidy display order, wrap with `sorted(...)`.

```{python}
metabric_patients.loc[:, '3-gene_classifier'].dropna().unique()
# Tip: sorted(...) if you want them ordered for display
# sorted(metabric_patients.loc[:, '3-gene_classifier'].dropna().unique())
```


{{< include ../_includes/footer.qmd >}}

---
<table style="width: 100%; border: none;">
  <tr>
    <td style="text-align: left; width: 50%;">
        [← Previous](../vignettes/4_Analysing_Data.qmd)
    </td>
    <td style="text-align: right; width: 50%;">
      [Next →](../vignettes/5_Visualising_Data.qmd)
    </td>
  </tr>
</table>


